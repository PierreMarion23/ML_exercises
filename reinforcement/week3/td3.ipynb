{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning - TME 3 - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif du TME est d'implémenter les algorithmes de renforcement value-based étudiés en cours (Q-learning et ses variantes) et de les tester dans un framework classique (gym de open-ai, MDP GridWorld)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.sparse import dok_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation des algorithmes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trois agents sont implémentés :\n",
    "+ RandomAgent : un agent aléatoire\n",
    "+ PolicyIteration : un agent qui implémente l'algorithme Policy Iteration\n",
    "+ ValueIteration : un agent qui implémente l'algorithme Value Iteration\n",
    "\n",
    "Pour Policy Iteration et Value Iteration, on code une première implémentation naïve qui utilise la structure de données sous forme de dictionnaire. Cette implémentation naïve fonctionne en temps raisonnable pour tous les exemples donnés sauf l'exemple 9 (qui possède environ 200 000 états). Pour gérer cet exemple, on fait une implémentation plus sophistiquée utilisant des matrices sparse scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "env.setPlan(\"gridworldPlans/plan1.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "statedic, mdp = env.getMDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, env, n_actions, alpha, gamma, tau=1):\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        return np.random.randint(0, self.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent(object):\n",
    "    \"\"\"Agent implementing Value Iteration. Naive implementation with dictionary structure.\"\"\"\n",
    "\n",
    "    def __init__(self, env, n_actions, alpha, gamma, tau=1):\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.statedic, self.mdp = env.getMDP()\n",
    "        self.policy = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            if state in self.mdp:\n",
    "                list_actions = self.mdp[state].keys()\n",
    "                self.policy[state_id] = self.action_space.sample()\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        try:\n",
    "            return self.policy[self.statedic[self.env.state2str(observation)]]\n",
    "        except:\n",
    "            return 0\n",
    "                \n",
    "    def train(self, eps=5e-4, gamma=0.99):  # Value Iteration algorithm\n",
    "        value = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            value[state_id] = 0\n",
    "            \n",
    "        distance = np.inf\n",
    "        while distance > eps:\n",
    "            new_value = {}\n",
    "            \n",
    "            for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    new_value[state_id] = np.max(results)\n",
    "                else:\n",
    "                    new_value[state_id] = value[state_id]\n",
    "                    \n",
    "            distance = np.linalg.norm(np.array(list(value.values()))-np.array(list(new_value.values())), ord=np.inf)\n",
    "            value = new_value\n",
    "                    \n",
    "        for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    self.policy[state_id] = np.argmax(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning(object):\n",
    "    \"\"\"Implementing a Q learning agent with Boltzmann selection\"\"\"\n",
    "\n",
    "    def __init__(self, env, n_actions, alpha, gamma, tau=1):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros((100, n_actions))\n",
    "        self.n_actions = n_actions\n",
    "        self.action = 0\n",
    "        self.state = 0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.dic_state = {}\n",
    "        self.nS = 0\n",
    "        self.first = True\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        if self.first:\n",
    "            self.first = False\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        \n",
    "        if self.env.state2str(observation) in self.dic_state:\n",
    "            new_state = self.dic_state[self.env.state2str(observation)]\n",
    "        else:\n",
    "            new_state = len(self.dic_state)\n",
    "            self.dic_state[self.env.state2str(observation)] = len(self.dic_state)\n",
    "            \n",
    "            \n",
    "        if not done:\n",
    "            self.Q[self.state][self.action] = self.Q[self.state][self.action] + self.alpha * (reward + self.gamma * np.max(self.Q[new_state]) - self.Q[self.state][self.action])\n",
    "        else:\n",
    "            self.Q[self.state][self.action] = self.Q[self.state][self.action] + self.alpha * (reward - self.Q[self.state][self.action])\n",
    "        \n",
    "        self.state = new_state\n",
    "        # boltzmann\n",
    "        probas = np.exp(self.Q[self.state] / self.tau)\n",
    "        probas = probas / np.sum(probas)\n",
    "        self.action = np.random.choice(range(self.n_actions), p=probas)\n",
    "        \n",
    "        return self.action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ_learning(object):\n",
    "    \"\"\"Implementing a Dyna-Q learning agent with Boltzmann selection\"\"\"\n",
    "\n",
    "    def __init__(self, env, n_actions, alpha, gamma, tau=1):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros((11, n_actions))\n",
    "        self.R = np.zeros((11, n_actions, 11))\n",
    "        self.P = np.zeros((11, n_actions, 11))\n",
    "        self.n_actions = n_actions\n",
    "        self.action = 0\n",
    "        self.state = 0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.dic_state = {}\n",
    "        self.nS = 0\n",
    "        self.first = True\n",
    "        self.done = False\n",
    "        self.c = 0\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "   #     if self.first:\n",
    "   #         self.first = False\n",
    "   #         return np.random.choice(range(self.n_actions))\n",
    "        \n",
    "    \n",
    "        if self.env.state2str(observation) in self.dic_state:\n",
    "            new_state = self.dic_state[self.env.state2str(observation)]\n",
    "        else:\n",
    "            new_state = len(self.dic_state)\n",
    "            self.dic_state[self.env.state2str(observation)] = len(self.dic_state)\n",
    "             \n",
    "        if new_state == 0 and reward > 0:\n",
    "            print(self.state)\n",
    "            print(done)\n",
    "            print(self.env.state2str(observation))\n",
    "            raise ValueError('non')\n",
    "            \n",
    "        if not self.done:\n",
    "            if not done:\n",
    "                self.Q[self.state][self.action] = self.Q[self.state][self.action] + self.alpha * (reward + self.gamma * np.max(self.Q[new_state]) - self.Q[self.state][self.action])\n",
    "            if done:\n",
    "                self.Q[self.state][self.action] = self.Q[self.state][self.action] + self.alpha * (reward - self.Q[self.state][self.action])\n",
    "\n",
    "            alphaR = 0.2\n",
    "            \n",
    "            self.R[self.state][self.action][new_state] = (1-alphaR)*self.R[self.state][self.action][new_state] + alphaR*reward\n",
    "            self.P[self.state][self.action] = (1-alphaR)*self.P[self.state][self.action]\n",
    "            self.P[self.state][self.action][new_state] = self.P[self.state][self.action][new_state] + alphaR\n",
    "\n",
    "            k = 10\n",
    "            for i in range(k):\n",
    "                s = np.random.randint(0, len(self.dic_state))\n",
    "                a = np.random.randint(0, self.n_actions)\n",
    "                for s2 in range(len(self.dic_state)):\n",
    "                    if self.R[s][a][s2] + self.gamma * np.max(self.Q[s2]) > 1:\n",
    "                        print(s, a, s2)\n",
    "                        for k,v in self.dic_state.items():\n",
    "                            if v == s2:\n",
    "                                print('destiniation')\n",
    "                                print(k)\n",
    "                            if v == s:\n",
    "                                print('origine')\n",
    "                                print(k)\n",
    "                        print(self.R[s][a][s2])\n",
    "                        print(self.R)\n",
    "                        print(np.max(self.Q[s2]))\n",
    "                        raise ValueError('oh oh')\n",
    "                self.Q[s][a] = (1 - self.alpha)*self.Q[s][a] + self.alpha*np.sum([self.P[s][a][s2]*(self.R[s][a][s2] + self.gamma * np.max(self.Q[s2])) for s2 in range(len(self.dic_state))])\n",
    "\n",
    "        self.state = new_state\n",
    "            \n",
    "        self.done = done\n",
    "        # boltzmann\n",
    "        try:\n",
    "            probas = np.exp(self.Q[self.state] / self.tau)\n",
    "            probas = probas / np.sum(probas)\n",
    "            self.action = np.random.choice(range(self.n_actions), p=probas)\n",
    "        except:\n",
    "            print(self.Q[self.state])\n",
    "            raise ValueError('ot')\n",
    "        \n",
    "        \n",
    "        return self.action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(object):\n",
    "    \"\"\"Implementing a SARSA agent with Boltzmann selection\"\"\"\n",
    "\n",
    "    def __init__(self, env, n_actions, alpha, gamma, tau=1):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros((100, n_actions))\n",
    "        self.n_actions = n_actions\n",
    "        self.action = 0\n",
    "        self.state = 0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.dic_state = {}\n",
    "        self.nS = 0\n",
    "        self.first = True\n",
    "        self.epochs = 1\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "     #   if self.first:\n",
    "     #       self.first = False\n",
    "     #       return np.random.choice(range(self.n_actions))\n",
    "        \n",
    "        if self.env.state2str(observation) in self.dic_state:\n",
    "            new_state = self.dic_state[self.env.state2str(observation)]\n",
    "        else:\n",
    "            \n",
    "            new_state = len(self.dic_state)\n",
    "            self.dic_state[self.env.state2str(observation)] = len(self.dic_state)\n",
    "            \n",
    "            \n",
    "        # boltzmann\n",
    "       # probas = np.exp(self.Q[self.state] / self.tau)\n",
    "       # probas = probas / np.sum(probas)\n",
    "        # eps greedy\n",
    "        eps = 0.1 / np.sqrt(self.epochs)\n",
    "        if np.random.random_sample() < eps:\n",
    "            new_action = np.random.choice(range(self.n_actions))\n",
    "            if self.epochs > 4000:\n",
    "                print('exploration choice')\n",
    "        else:\n",
    "            new_action = np.argmax(self.Q[new_state])\n",
    "            \n",
    "        if not done:\n",
    "            self.Q[self.state][self.action] = self.Q[self.state][self.action] + self.alpha * (reward + self.gamma * self.Q[new_state][new_action] - self.Q[self.state][self.action])\n",
    "        else:\n",
    "            self.epochs += 1\n",
    "            self.Q[self.state][self.action] = self.Q[self.state][self.action] + self.alpha * (reward - self.Q[self.state][self.action])\n",
    "        \n",
    "        self.state = new_state\n",
    "        self.action = new_action\n",
    "        \n",
    "        return self.action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur la plateforme gym - problème GridWorld "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par encapsuler le protocole de test dans des fonctions. Ces fonctions techniques ne présentent pas d'intérêt algorithmique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_number, rewards):\n",
    "    env = gym.make(\"gridworld-v0\")\n",
    "    env.setPlan(\"gridworldPlans/plan\" + str(env_number) + \".txt\", rewards)\n",
    "    env.seed(0)  # Initialise le seed du pseudo-random\n",
    "            \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'gridworld-v0/agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "for i in range(n_runs):\n",
    "    obs = envm.reset()\n",
    "    reward = 0 \n",
    "    done = False \n",
    "    env.render(0.1)\n",
    "    while True:\n",
    "        action = agent.act(obs, reward, done)\n",
    "        if done:\n",
    "            break\n",
    "        obs, reward, done, _ = envm.step(action) \n",
    "        env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, n_runs=10, verbose=1):\n",
    "    outdir = 'gridworld-v0/agent-results'\n",
    "    envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "    reward = 0\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    rtot = []\n",
    "    for i in range(n_runs):\n",
    "        obs = envm.reset()\n",
    "        reward = 0 ####\n",
    "        done = False ####\n",
    "        if verbose > 1:\n",
    "            env.render(0.1)\n",
    "        j = 0\n",
    "        rsum = 0\n",
    "        while True:\n",
    "            action = agent.act(obs, reward, done)\n",
    "            if done:\n",
    "                if verbose > 1:\n",
    "                    print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "                break\n",
    "            obs, reward, done, _ = envm.step(action) #####\n",
    "            rsum += reward\n",
    "            j += 1\n",
    "            if verbose > 1:\n",
    "                env.render()\n",
    "            \n",
    "        rtot.append(rsum)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Mean reward for agent %s over %d runs: %.2f\" % (agent.__class__.__name__, n_runs, np.mean(np.array(rtot))))\n",
    "    env.close()\n",
    "    return rtot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env_number, rewards, alpha, gamma, tau, AgentClass=Q_learning, run_exp=True, n_runs=10, verbose=3):\n",
    "    env = create_env(env_number, rewards)\n",
    "    agent = AgentClass(env, env.nA, alpha, gamma, tau)\n",
    "    if AgentClass == ValueIterationAgent:\n",
    "        agent.train()\n",
    "    if verbose:\n",
    "        print('Test - Env %d - %s - %d runs' % (env_number, AgentClass.__name__, n_runs))\n",
    "    if verbose > 2:\n",
    "        print('Policy of ' + agent.__class__.__name__ +  ': ' +str(agent.policy))\n",
    "    if run_exp:\n",
    "        rtot = run_experiment(env, agent, n_runs, verbose)\n",
    "    if verbose:\n",
    "        print(' ')\n",
    "    return rtot, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut comparer le reward moyen de l'agent optimal à celui de l'agent aléatoire. Comme prévu, l'agent optimal fait mieux que l'agent aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fe5516c6ba20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQ_learning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_exp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_runs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-0f3509795e16>\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(env_number, rewards, alpha, gamma, tau, AgentClass, run_exp, n_runs, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Policy of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_exp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9d6ff02dea0e>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(env, agent, n_runs, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mrsum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-412bfc67902c>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, observation, reward, done)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# boltzmann\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "taus = [0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "results = np.zeros((len(alphas), len(taus)))\n",
    "for i in range(len(alphas)):\n",
    "    for j in range(len(taus)):\n",
    "        results[i][j] = np.mean(np.array([test_agent(1, rewards, alphas[i], 0.99, taus[j], AgentClass=Q_learning, run_exp=True, n_runs=1000, verbose=0)[0]]*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "taus = [0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "results_SARSA = np.zeros((len(alphas), len(taus)))\n",
    "for i in range(len(alphas)):\n",
    "    for j in range(len(taus)):\n",
    "        results_SARSA[i][j] = np.mean(np.array([test_agent(1, rewards, alphas[i], 0.99, taus[j], AgentClass=SARSA, run_exp=True, n_runs=1000, verbose=0)[0]]*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.693355, 1.726217, 1.289149, 1.258777, 1.085849],\n",
       "       [1.936201, 1.421532, 1.334497, 1.348095, 1.348891],\n",
       "       [1.557488, 1.630154, 1.443283, 1.376981, 1.335862],\n",
       "       [1.825666, 1.562489, 1.558177, 1.586178, 1.10239 ],\n",
       "       [1.951264, 1.954646, 1.959491, 1.946639, 1.569964],\n",
       "       [1.857181, 1.854962, 1.867367, 1.858754, 1.869576]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.676402,  0.658348,  0.185805,  0.014817,  0.013005],\n",
       "       [ 0.590284,  0.565209,  0.406066,  0.191175,  0.157219],\n",
       "       [ 0.503001,  0.513095,  0.446625,  0.240555,  0.067447],\n",
       "       [ 0.515446,  0.513229,  0.375796,  0.257763,  0.099999],\n",
       "       [ 0.444595,  0.319243,  0.408213,  0.266893,  0.165366],\n",
       "       [-0.1436  , -0.113914, -0.098177, -0.175775, -0.082476]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Env 0 - ValueIterationAgent - 1000 runs\n",
      "Mean reward for agent ValueIterationAgent over 1000 runs: 0.98\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r_best, a_best = test_agent(0, rewards, 0.1, 0.99, 0.01, AgentClass=ValueIterationAgent, run_exp=True, n_runs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Env 0 - RandomAgent - 100 runs\n",
      "Mean reward for agent RandomAgent over 100 runs: -0.73\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r_random = test_agent(0, rewards, 0.1, 0.99, 0.1, AgentClass=RandomAgent, run_exp=True, n_runs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Env 0 - Q_learning - 1000 runs\n",
      "Mean reward for agent Q_learning over 1000 runs: 0.96\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r_q, a_q = test_agent(0, rewards, 0.5, 0.99, 0.005, AgentClass=Q_learning, run_exp=True, n_runs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Env 0 - DynaQ_learning - 1000 runs\n",
      "Mean reward for agent DynaQ_learning over 1000 runs: 0.95\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r_dynaq, a_dynaq = test_agent(0, rewards, 0.5, 0.99, 0.01, AgentClass=DynaQ_learning, run_exp=True, n_runs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84323308, -0.04      ,  0.4632109 ,  0.39177339],\n",
       "       [ 0.58754653,  0.90400915,  0.68841289,  0.7524059 ],\n",
       "       [ 0.69708818,  0.31907755,  0.56699352,  0.87366154],\n",
       "       [ 0.51448633,  0.36278006,  0.43051372,  0.77413447],\n",
       "       [ 0.54447938,  0.        ,  0.40658246,  0.41437358],\n",
       "       [ 0.42872979,  0.32722888,  0.95040964, -0.04      ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.22578476,  0.24554285,  0.38063976,  0.26410808],\n",
       "       [ 0.31904207,  0.22355225,  0.31123707,  0.11292418],\n",
       "       [ 0.32064457,  0.        ,  0.        ,  0.99404439],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dynaq.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.660393640650938"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(a_dynaq.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for agent DynaQ_learning over 100 runs: 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.981,\n",
       " 0.8649999999999999,\n",
       " 0.969,\n",
       " 0.986,\n",
       " 0.972,\n",
       " 0.964,\n",
       " 0.991,\n",
       " 0.946,\n",
       " 0.969,\n",
       " 0.974,\n",
       " 0.944,\n",
       " 0.978,\n",
       " 0.967,\n",
       " 0.97,\n",
       " 0.969,\n",
       " 0.966,\n",
       " 0.9289999999999999,\n",
       " 0.973,\n",
       " 0.975,\n",
       " 0.947,\n",
       " 0.974,\n",
       " 0.984,\n",
       " 0.968,\n",
       " 0.954,\n",
       " 0.951,\n",
       " 0.954,\n",
       " 0.983,\n",
       " 0.971,\n",
       " 0.976,\n",
       " 0.939,\n",
       " 0.954,\n",
       " 0.955,\n",
       " 0.958,\n",
       " 0.943,\n",
       " 0.966,\n",
       " 0.966,\n",
       " 0.97,\n",
       " 0.977,\n",
       " 0.98,\n",
       " 0.954,\n",
       " 0.964,\n",
       " 0.993,\n",
       " 0.971,\n",
       " 0.95,\n",
       " 0.98,\n",
       " 0.967,\n",
       " 0.994,\n",
       " 0.976,\n",
       " 0.9129999999999999,\n",
       " 0.973,\n",
       " 0.991,\n",
       " 0.98,\n",
       " 0.982,\n",
       " 0.976,\n",
       " 0.966,\n",
       " 0.993,\n",
       " 0.9179999999999999,\n",
       " 0.948,\n",
       " 0.973,\n",
       " 0.982,\n",
       " 0.946,\n",
       " 0.9339999999999999,\n",
       " 0.975,\n",
       " 0.974,\n",
       " 0.9309999999999999,\n",
       " 0.951,\n",
       " 0.8939999999999999,\n",
       " 0.968,\n",
       " 0.949,\n",
       " 0.962,\n",
       " 0.9249999999999999,\n",
       " 0.9259999999999999,\n",
       " 0.991,\n",
       " 0.9319999999999999,\n",
       " 0.958,\n",
       " 0.99,\n",
       " 0.946,\n",
       " 0.99,\n",
       " 0.98,\n",
       " 0.9219999999999999,\n",
       " 0.9089999999999999,\n",
       " 0.8999999999999999,\n",
       " 0.971,\n",
       " 0.9369999999999999,\n",
       " 0.986,\n",
       " 0.976,\n",
       " 0.975,\n",
       " 0.981,\n",
       " 0.974,\n",
       " 0.949,\n",
       " 0.973,\n",
       " 0.976,\n",
       " 0.962,\n",
       " 0.968,\n",
       " 0.96,\n",
       " 0.976,\n",
       " 0.9319999999999999,\n",
       " 0.8969999999999999,\n",
       " 0.8349999999999999,\n",
       " 0.938]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "env.setPlan(\"gridworldPlans/plan0.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "run_experiment(env, a_dynaq, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Env 0 - SARSA - 2000 runs\n",
      "Mean reward for agent SARSA over 2000 runs: 0.91\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r_sarsa, a_sarsa = test_agent(0, rewards, 0.001, 0.99, 0.01, AgentClass=SARSA, run_exp=True, n_runs=2000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814469809467841"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(a_sarsa.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "exploration choice\n",
      "Mean reward for agent SARSA over 1000 runs: 1.95\n"
     ]
    }
   ],
   "source": [
    "_ = run_experiment(env, a_sarsa, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 2: 1, 3: 2, 4: 1, 5: 0, 7: 3, 8: 1, 9: 0, 10: 3, 11: 3, 12: 3, 14: 3, 15: 2, 16: 2, 17: 1, 18: 1, 19: 3}\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for key, value in a_best.statedic.items():\n",
    "    if key in a_sarsa.dic_state:\n",
    "        dic[value] = np.argmax(a_sarsa.Q, axis=1)[a_sarsa.dic_state[key]]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a007a461a69d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3300\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3302\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[0;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mmanagers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmanagers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mmanagers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(range(len(r_sarsa)), np.cumsum(np.array(r_sarsa)))\n",
    "matplotlib.pyplot.plot(range(len(r_best)), np.cumsum(np.array(r_best)))\n",
    "matplotlib.pyplot.plot(range(len(r_q)), np.cumsum(np.array(r_q)))\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les environnements 4 et 7, un phénomène intéressant se produit : il faut passer par un malus pour arriver à la sortie. Dans ces cas, il faut bien choisir la combinaison de gamma et du reward des cases vides (`reward[0]`) pour que l'agent apprenne. Par exemple, la combinaison `reward[0] = -0.001` et `gamma=0.99` ne fonctionne pas. Les combinaisons `reward[0] = -0.01` et `gamma=0.99` ou `reward[0] = -0.001` et `gamma=1` fonctionnent (mais donnent des comportements plus ou moins prudents dans le cas de l'environnement 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(4, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}, run_exp=True, n_runs=1, verbose=2, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 7 - ValueIterationAgent - 5 runs\n",
      "Episode : 0 rsum=2.4699999999999993, 58 actions\n",
      "Episode : 1 rsum=2.4799999999999995, 57 actions\n",
      "Episode : 2 rsum=-0.07000000000000006, 9 actions\n",
      "Episode : 3 rsum=0.8999999999999999, 13 actions\n",
      "Episode : 4 rsum=0.8799999999999999, 15 actions\n",
      "Mean reward for agent ValueIterationAgent over 5 runs: 1.33\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(7, {0: -0.01, 3: 1, 4: 1, 5: -1, 6: -1}, run_exp=True, n_runs=5, verbose=2, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, pour l'environnement 9, on utilise l'agent OptimizedValueIterationAgent. L'exécution du test nécessite environ 2 minutes et 4 GB de mémoire vive. L'essentiel de ce budget est consacré à la construction du MDP et sa traduction sous la bonne forme de données (matrices sparse). La phase d'entraînement à proprement parler est très rapide (~ 10 secondes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 9 - OptimizedValueIterationAgent - 1 runs\n",
      "Episode : 0 rsum=3.850000000000006, 122 actions\n",
      "Mean reward for agent OptimizedValueIterationAgent over 1 runs: 3.85\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(9, {0: -0.01, 3: 1, 4: 1, 5: -1, 6: -1}, AgentClass=OptimizedValueIterationAgent, run_exp=True, n_runs=2, verbose=2, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
