{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning - TME 2 - Programmation Dynamique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif du TME est d'implémenter les algorithmes de programmation dynamique étudiés en cours (Policy Iteration et Value Iteration) et de les tester dans un framework classique (gym de open-ai, MDP GridWorld)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.sparse import dok_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For exemple 9, to construct recursively the MDP, we need to increase the recursion depth.\n",
    "import sys\n",
    "sys.setrecursionlimit(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation des algorithmes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trois agents sont implémentés :\n",
    "+ RandomAgent : un agent aléatoire\n",
    "+ PolicyIteration : un agent qui implémente l'algorithme Policy Iteration\n",
    "+ ValueIteration : un agent qui implémente l'algorithme Value Iteration\n",
    "\n",
    "Pour Policy Iteration et Value Iteration, on code une première implémentation naïve qui utilise la structure de données sous forme de dictionnaire. Cette implémentation naïve fonctionne en temps raisonnable pour tous les exemples donnés sauf l'exemple 9 (qui possède environ 200 000 états). Pour gérer cet exemple, on fait une implémentation plus sophistiquée utilisant des matrices sparse scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "env.setPlan(\"gridworldPlans/plan1.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "statedic, mdp = env.getMDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.policy = 'Random policy'\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def train(self, eps, gamma):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent(object):\n",
    "    \"\"\"Agent implementing Policy Iteration\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.statedic, self.mdp = env.getMDP()\n",
    "        self.policy = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            if state in self.mdp:\n",
    "                list_actions = self.mdp[state].keys()\n",
    "                self.policy[state_id] = self.action_space.sample()\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.policy[self.statedic[self.env.state2str(observation)]]\n",
    "    \n",
    "    def value_evaluation(self, eps=5e-4, gamma=0.99):\n",
    "        value = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            value[state_id] = 0\n",
    "            \n",
    "        distance = np.inf\n",
    "        while distance > eps:\n",
    "            new_value = {}\n",
    "            for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    new_value[state_id] = sum([proba*(reward + gamma*value[self.statedic[new_state]]) for proba, new_state, reward, done in self.mdp[state][self.policy[state_id]]])\n",
    "                else:\n",
    "                    new_value[state_id] = value[state_id]\n",
    "            \n",
    "            distance = np.linalg.norm(np.array(list(value.values()))-np.array(list(new_value.values())), ord=np.inf)\n",
    "            value = new_value\n",
    "        self.value = value\n",
    "                \n",
    "    def train(self, eps=5e-4, gamma=0.99):   # Policy Iteration algorithm\n",
    "        policy_changed = True\n",
    "        \n",
    "        while policy_changed:\n",
    "            \n",
    "            policy_changed = False\n",
    "            \n",
    "            self.value_evaluation(eps, gamma)\n",
    "            new_policy = {}\n",
    "            \n",
    "            for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*self.value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    new_policy[state_id] = np.argmax(results)\n",
    "                    if new_policy[state_id] != self.policy[state_id]:\n",
    "                        policy_changed = True\n",
    "            \n",
    "            self.policy = new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent(object):\n",
    "    \"\"\"Agent implementing Value Iteration. Naive implementation with dictionary structure.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.statedic, self.mdp = env.getMDP()\n",
    "        self.policy = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            if state in self.mdp:\n",
    "                list_actions = self.mdp[state].keys()\n",
    "                self.policy[state_id] = self.action_space.sample()\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.policy[self.statedic[self.env.state2str(observation)]]\n",
    "                \n",
    "    def train(self, eps=5e-4, gamma=0.99):  # Value Iteration algorithm\n",
    "        value = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            value[state_id] = 0\n",
    "            \n",
    "        distance = np.inf\n",
    "        while distance > eps:\n",
    "            new_value = {}\n",
    "            \n",
    "            for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    new_value[state_id] = np.max(results)\n",
    "                else:\n",
    "                    new_value[state_id] = value[state_id]\n",
    "                    \n",
    "            distance = np.linalg.norm(np.array(list(value.values()))-np.array(list(new_value.values())), ord=np.inf)\n",
    "            value = new_value\n",
    "                    \n",
    "        for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    self.policy[state_id] = np.argmax(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedValueIterationAgent(object):\n",
    "    \"\"\"Agent implementing Value Iteration with an efficient implementation (scipy sparse matrices).\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.statedic, self.mdp = env.getMDP()\n",
    "        self.policy = np.zeros(len(self.statedic.items()))\n",
    "        \n",
    "        # Translation of the MDP as CSC scipy matrices\n",
    "        # See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix\n",
    "        rewards = [lil_matrix((env.nS, env.nS)) for a in range(env.nA)]\n",
    "        probas = [lil_matrix((env.nS, env.nS)) for a in range(env.nA)]\n",
    "        for state in self.mdp.keys():\n",
    "            for action, transitions in self.mdp[state].items():\n",
    "                for (proba, new_state, reward, done) in transitions:\n",
    "                    rewards[action][self.statedic[state], self.statedic[new_state]] = reward\n",
    "                    probas[action][self.statedic[state], self.statedic[new_state]] += proba\n",
    "                    \n",
    "        self.rewards = [x.tocsc() for x in rewards]\n",
    "        self.probas = [x.tocsc() for x in probas]\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.policy[self.statedic[self.env.state2str(observation)]]\n",
    "                \n",
    "    def train(self, eps=5e-4, gamma=0.99):\n",
    "        nS = self.env.nS\n",
    "        nA = self.env.nA\n",
    "        value = np.zeros(nS)\n",
    "        distance = np.inf\n",
    "        while distance > eps:\n",
    "            new_value = np.zeros(nS)\n",
    "\n",
    "            action_values = np.zeros((nS, nA))\n",
    "            for a in range(nA):\n",
    "                reward = self.rewards[a].copy()\n",
    "                proba = self.probas[a]\n",
    "                \n",
    "                # create the array of gamma*value of the appropriate shape to be added to the rewards\n",
    "                delta = np.repeat(gamma*value, np.diff(reward.indptr))\n",
    "                reward.data += delta\n",
    "                action_values[:, a] = (proba.multiply(reward)).sum(axis=1).flatten()\n",
    "\n",
    "            new_value = np.max(action_values, axis=1)\n",
    "\n",
    "            distance = np.linalg.norm(new_value-value, ord=np.inf)\n",
    "            value = new_value\n",
    "\n",
    "        action_values = np.zeros((nS, nA))\n",
    "        for a in range(nA):\n",
    "            reward = self.rewards[a].copy()\n",
    "            proba = self.probas[a].copy()\n",
    "\n",
    "            delta = np.repeat(gamma*value, np.diff(reward.indptr))\n",
    "            reward.data += delta\n",
    "            action_values[:, a] = (proba.multiply(reward)).sum(axis=1).flatten()\n",
    "\n",
    "\n",
    "        self.policy = np.argmax(action_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur la plateforme gym - problème GridWorld "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par encapsuler le protocole de test dans des fonctions. Ces fonctions techniques ne présentent pas d'intérêt algorithmique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_number, rewards):\n",
    "    env = gym.make(\"gridworld-v0\")\n",
    "    env.setPlan(\"gridworldPlans/plan\" + str(env_number) + \".txt\", rewards)\n",
    "    env.seed(0)  # Initialise le seed du pseudo-random\n",
    "            \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, n_runs=10, verbose=1):\n",
    "    outdir = 'gridworld-v0/agent-results'\n",
    "    envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "    reward = 0\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    rtot = 0\n",
    "    for i in range(n_runs):\n",
    "        obs = envm.reset()\n",
    "        if verbose > 1:\n",
    "            env.render(0.1)\n",
    "        j = 0\n",
    "        rsum = 0\n",
    "        while True:\n",
    "            action = agent.act(obs, reward, done)\n",
    "            obs, reward, done, _ = envm.step(action)\n",
    "            rsum += reward\n",
    "            j += 1\n",
    "            if verbose > 1:\n",
    "                env.render()\n",
    "            if done:\n",
    "                if verbose > 1:\n",
    "                    print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "                break\n",
    "        rtot += rsum\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Mean reward for agent %s over %d runs: %.2f\" % (agent.__class__.__name__, n_runs, rtot/n_runs))\n",
    "    env.close()\n",
    "    return rtot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env_number, rewards, gamma=0.99, AgentClass=ValueIterationAgent, run_exp=True, n_runs=10, verbose=3, eps=5e-4):\n",
    "    env = create_env(env_number, rewards)\n",
    "    agent = AgentClass(env)\n",
    "    if verbose > 1:\n",
    "        print('MDP loaded')\n",
    "    agent.train(eps=eps, gamma=gamma)\n",
    "    if verbose:\n",
    "        print('Test - Env %d - %s - %d runs' % (env_number, AgentClass.__name__, n_runs))\n",
    "    if verbose > 2:\n",
    "        print('Policy of ' + agent.__class__.__name__ +  ': ' +str(agent.policy))\n",
    "    if run_exp:\n",
    "        run_experiment(env, agent, n_runs, verbose)\n",
    "    if verbose:\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que Policy Iteration et Value Iteration ont la même policy. On affiche 10 runs pour vérifier que l'agent a bien le comportement attendu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 0 - ValueIterationAgent - 10 runs\n",
      "Policy of ValueIterationAgent: {0: 0, 2: 2, 3: 2, 4: 3, 6: 3, 7: 3, 8: 1, 9: 1, 10: 2}\n",
      " \n",
      "MDP loaded\n",
      "Test - Env 0 - PolicyIterationAgent - 5 runs\n",
      "Policy of PolicyIterationAgent: {0: 0, 2: 2, 3: 2, 4: 3, 6: 3, 7: 3, 8: 1, 9: 1, 10: 2}\n",
      "Episode : 0 rsum=0.974, 27 actions\n",
      "Episode : 1 rsum=0.983, 18 actions\n",
      "Episode : 2 rsum=0.982, 19 actions\n",
      "Episode : 3 rsum=0.986, 15 actions\n",
      "Episode : 4 rsum=0.989, 12 actions\n",
      "Mean reward for agent PolicyIterationAgent over 5 runs: 0.98\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(0, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}, AgentClass=ValueIterationAgent, run_exp=False)\n",
    "test_agent(0, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}, AgentClass=PolicyIterationAgent, run_exp=True, n_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant les rewards (-0.4 pour un mouvement sur une case vide au lieu de -0.001), on observe que la policy optimale change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 0 - ValueIterationAgent - 5 runs\n",
      "Policy of ValueIterationAgent: {0: 2, 2: 1, 3: 1, 4: 3, 6: 3, 7: 3, 8: 1, 9: 1, 10: 3}\n",
      "Episode : 0 rsum=-0.20000000000000018, 4 actions\n",
      "Episode : 1 rsum=-0.20000000000000018, 4 actions\n",
      "Episode : 2 rsum=-0.20000000000000018, 4 actions\n",
      "Episode : 3 rsum=-1.7999999999999998, 8 actions\n",
      "Episode : 4 rsum=-1, 1 actions\n",
      "Mean reward for agent ValueIterationAgent over 5 runs: -0.68\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(0, {0: -0.4, 3: 1, 4: 1, 5: -1, 6: -1}, run_exp=True, n_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut comparer le reward moyen de l'agent optimal à celui de l'agent aléatoire. Comme prévu, l'agent optimal fait mieux que l'agent aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Env 0 - ValueIterationAgent - 1000 runs\n",
      "Mean reward for agent ValueIterationAgent over 1000 runs: 0.98\n",
      " \n",
      "Test - Env 0 - RandomAgent - 1000 runs\n",
      "Mean reward for agent RandomAgent over 1000 runs: -0.75\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(0, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}, run_exp=True, n_runs=1000, verbose=1)\n",
    "test_agent(0, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}, AgentClass=RandomAgent, run_exp=True, n_runs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les environnements 4 et 7, un phénomène intéressant se produit : il faut passer par un malus pour arriver à la sortie. Dans ces cas, il faut bien choisir la combinaison de gamma et du reward des cases vides (`reward[0]`) pour que l'agent apprenne. Par exemple, la combinaison `reward[0] = -0.001` et `gamma=0.99` ne fonctionne pas. Les combinaisons `reward[0] = -0.01` et `gamma=0.99` ou `reward[0] = -0.001` et `gamma=1` fonctionnent (mais donnent des comportements plus ou moins prudents dans le cas de l'environnement 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 4 - ValueIterationAgent - 1 runs\n",
      "Episode : 0 rsum=-0.02599999999999758, 28 actions\n",
      "Mean reward for agent ValueIterationAgent over 1 runs: -0.03\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(4, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}, run_exp=True, n_runs=1, verbose=2, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 7 - ValueIterationAgent - 5 runs\n",
      "Episode : 0 rsum=2.4699999999999993, 58 actions\n",
      "Episode : 1 rsum=2.4799999999999995, 57 actions\n",
      "Episode : 2 rsum=-0.07000000000000006, 9 actions\n",
      "Episode : 3 rsum=0.8999999999999999, 13 actions\n",
      "Episode : 4 rsum=0.8799999999999999, 15 actions\n",
      "Mean reward for agent ValueIterationAgent over 5 runs: 1.33\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(7, {0: -0.01, 3: 1, 4: 1, 5: -1, 6: -1}, run_exp=True, n_runs=5, verbose=2, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, pour l'environnement 9, on utilise l'agent OptimizedValueIterationAgent. L'exécution du test nécessite environ 2 minutes et 4 GB de mémoire vive. L'essentiel de ce budget est consacré à la construction du MDP et sa traduction sous la bonne forme de données (matrices sparse). La phase d'entraînement à proprement parler est très rapide (~ 10 secondes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP loaded\n",
      "Test - Env 9 - OptimizedValueIterationAgent - 2 runs\n",
      "Episode : 0 rsum=4.510000000000008, 155 actions\n",
      "Episode : 1 rsum=4.730000000000006, 133 actions\n",
      "Mean reward for agent OptimizedValueIterationAgent over 2 runs: 4.62\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_agent(9, {0: -0.01, 3: 1, 4: 1, 5: -1, 6: -1}, AgentClass=OptimizedValueIterationAgent, run_exp=True, n_runs=2, verbose=2, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
