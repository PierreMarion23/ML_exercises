{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.sparse import dok_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent(object):\n",
    "    \"\"\"Agent implementing Value Iteration. Naive implementation with dictionary structure.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.statedic, self.mdp = env.getMDP()\n",
    "        self.policy = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            if state in self.mdp:\n",
    "                list_actions = self.mdp[state].keys()\n",
    "                self.policy[state_id] = self.action_space.sample()\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.policy[self.statedic[self.env.state2str(observation)]]\n",
    "                \n",
    "    def train(self, eps=5e-4, gamma=0.99):  # Value Iteration algorithm\n",
    "        value = {}\n",
    "        for state, state_id in self.statedic.items():\n",
    "            value[state_id] = 0\n",
    "            \n",
    "        distance = np.inf\n",
    "        while distance > eps:\n",
    "            new_value = {}\n",
    "            \n",
    "            for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    new_value[state_id] = np.max(results)\n",
    "                else:\n",
    "                    new_value[state_id] = value[state_id]\n",
    "                    \n",
    "            distance = np.linalg.norm(np.array(list(value.values()))-np.array(list(new_value.values())), ord=np.inf)\n",
    "            value = new_value\n",
    "                    \n",
    "        for state, state_id in self.statedic.items():\n",
    "                if state in self.mdp:\n",
    "                    results = [sum([proba*(reward + gamma*value[self.statedic[new_state]]) for (proba, new_state, reward, done) in transitions]) for action, transitions in self.mdp[state].items()]\n",
    "                    self.policy[state_id] = np.argmax(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedValueIterationAgent(object):\n",
    "    \"\"\"Agent implementing Value Iteration with an efficient implementation (scipy sparse matrices).\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.statedic, self.mdp = env.getMDP()\n",
    "        self.policy = np.zeros(len(self.statedic.items()))\n",
    "        \n",
    "        # Translation of the MDP as CSC scipy matrices\n",
    "        # See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix\n",
    "        rewards = [lil_matrix((env.nS, env.nS)) for a in range(env.nA)]\n",
    "        probas = [lil_matrix((env.nS, env.nS)) for a in range(env.nA)]\n",
    "        for state in self.mdp.keys():\n",
    "            for action, transitions in self.mdp[state].items():\n",
    "                for (proba, new_state, reward, done) in transitions:\n",
    "                    rewards[action][self.statedic[state], self.statedic[new_state]] = reward\n",
    "                    probas[action][self.statedic[state], self.statedic[new_state]] += proba\n",
    "                    \n",
    "        self.rewards = [x.tocsc() for x in rewards]\n",
    "        self.probas = [x.tocsc() for x in probas]\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.policy[self.statedic[self.env.state2str(observation)]]\n",
    "                \n",
    "    def train(self, eps=5e-4, gamma=0.99):\n",
    "        nS = self.env.nS\n",
    "        nA = self.env.nA\n",
    "        value = np.zeros(nS)\n",
    "        distance = np.inf\n",
    "        while distance > eps:\n",
    "            new_value = np.zeros(nS)\n",
    "\n",
    "            action_values = np.zeros((nS, nA))\n",
    "            for a in range(nA):\n",
    "                reward = self.rewards[a].copy()\n",
    "                proba = self.probas[a]\n",
    "                \n",
    "                # create the array of gamma*value of the appropriate shape to be added to the rewards\n",
    "                delta = np.repeat(gamma*value, np.diff(reward.indptr))\n",
    "                reward.data += delta\n",
    "                action_values[:, a] = (proba.multiply(reward)).sum(axis=1).flatten()\n",
    "\n",
    "            new_value = np.max(action_values, axis=1)\n",
    "\n",
    "            distance = np.linalg.norm(new_value-value, ord=np.inf)\n",
    "            value = new_value\n",
    "\n",
    "        action_values = np.zeros((nS, nA))\n",
    "        for a in range(nA):\n",
    "            reward = self.rewards[a].copy()\n",
    "            proba = self.probas[a].copy()\n",
    "\n",
    "            delta = np.repeat(gamma*value, np.diff(reward.indptr))\n",
    "            reward.data += delta\n",
    "            action_values[:, a] = (proba.multiply(reward)).sum(axis=1).flatten()\n",
    "\n",
    "\n",
    "        self.policy = np.argmax(action_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "env.setPlan(\"gridworldPlans/plan0.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "env.seed(0)  # Initialise le seed du pseudo-random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentVI = ValueIterationAgent(env)\n",
    "agentVI.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 2: 2, 3: 2, 4: 3, 6: 3, 7: 3, 8: 1, 9: 1, 10: 2}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentVI.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentOVI = OptimizedValueIterationAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 3, 0, 3, 3, 1, 1, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentOVI.train()\n",
    "agentOVI.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "statedic, mdp = env.getMDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "0\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 2 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "1\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 2 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "2\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 2 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "3\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 2 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "4\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "5\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 2 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "6\n",
      "[[1 1 1 1 1 1]\n",
      " [1 2 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "7\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 2 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "8\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 2 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "9\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 2 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for key, value in statedic.items():\n",
    "    print(env.str2state(key))\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
