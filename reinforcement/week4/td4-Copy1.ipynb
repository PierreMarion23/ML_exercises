{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning - TME 4 - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif du TME est d'implémenter les algorithmes de renforcement value-based étudiés en cours (Q-learning et ses variantes) et de les tester dans un framework classique (gym de open-ai, MDP GridWorld)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation des algorithmes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return np.random.choice([0, 1])\n",
    "    \n",
    "    def learn(self, observation, reward, done):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=[]):\n",
    "        super(NN, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for x in layers:\n",
    "            self.layers.append(nn.Linear(inSize, x))\n",
    "            inSize = x\n",
    "        self.layers.append(nn.Linear(inSize, outSize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            x = torch.nn.functional.leaky_relu(x)\n",
    "            x = self.layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètres CartPole\n",
    "\n",
    "epsilon=0.01, epsilonDecay=0.99999\n",
    "gamma=0.999\n",
    "btachSize=100, capacity=100000\n",
    "ctarget=100\n",
    "layers=[200]\n",
    "lr=0.001\n",
    "\n",
    "LunarLander (convergence après environ 10000 episodes):\n",
    "epsilon=0.1, epsilonDecay=0.99999\n",
    "gamma=0.99\n",
    "btachSize=1, capacity=1\n",
    "ctarget=1000\n",
    "layers=[200]\n",
    "lr=0.0001\n",
    "\n",
    "  Pour Gridworld  (convergence après environ 2000 episodes sur plan0 avec rewards={0:-0.001,3:1,4:1,5:-1,6:-1}):\n",
    "epsilon=0.1, epsilonDecay=0.9999 (epsilon multiplié par epsilonDecay à chaque passage dans act)\n",
    "gamma=0.99\n",
    "batchSize=10, capacity=1000000\n",
    "ctarget=1000 (fréquence de mise à jour du réseau cible)\n",
    "layers=[30,30]\n",
    "lr=0.0001 (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    \"\"\"Implementing a DQN learning agent\"\"\"\n",
    "\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    "        self.N = params['N']\n",
    "        self.D = np.zeros((self.N, 4 + 1 + 1 + 4 + 1))\n",
    "        self.C = params['C']\n",
    "        self.batch = params['batch']\n",
    "        self.Q = NN(5, 1, params['layers'])\n",
    "        self.Qhat = NN(5, 1, params['layers'])\n",
    "        self.eps = params['eps']\n",
    "        self.epsDecay = params['epsDecay']\n",
    "        self.state = []\n",
    "        self.step = -1\n",
    "        self.gamma = params['gamma']\n",
    "        self.loss = torch.nn.SmoothL1Loss()\n",
    "        self.optim = torch.optim.Adam(params=self.Q.parameters(), lr=params['lr'])\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        self.state = observation\n",
    "        if np.random.rand() < self.eps:\n",
    "            self.action = np.random.choice([0, 1])\n",
    "        else:\n",
    "            input0 = torch.tensor(np.append(observation, [0])).float()\n",
    "            input1 = torch.tensor(np.append(observation, [1])).float()\n",
    "            self.action = 0 if self.Q(input0) > self.Q(input1) else 1\n",
    "            \n",
    "        self.eps *= self.epsDecay\n",
    "        return self.action\n",
    "    \n",
    "    def learn(self, observation, reward, done):\n",
    "        self.step += 1\n",
    "        self.D[self.step % self.N] = list(self.state) + [self.action, reward] + list(observation) + [int(done)]\n",
    "              \n",
    "        inputs = self.D[np.random.randint(0, min(self.N, self.step+1), self.batch)]\n",
    "        x = torch.from_numpy(inputs[:, 0:5]).float()\n",
    "      #  print(x)\n",
    "        rewards = inputs[:, 5]\n",
    "        \n",
    "        input0 = torch.from_numpy(np.append(inputs[:, 6:10], np.zeros((self.batch, 1)), axis=1)).float()\n",
    "        input1 = torch.from_numpy(np.append(inputs[:, 6:10], np.ones((self.batch, 1)), axis=1)).float()\n",
    "        y = rewards + self.gamma * (1-inputs[:, -1]) * np.max([self.Qhat(input0).detach().numpy(), self.Qhat(input1).detach().numpy()], axis=0).flatten()\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        l = self.loss(torch.from_numpy(y).float(), self.Q(x).flatten())\n",
    "        l.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "        if (self.step % self.C) == 0:\n",
    "            self.Qhat = copy.deepcopy(self.Q)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "11.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "11.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "11.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "11.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "16.0\n",
      "29.0\n",
      "10.0\n",
      "43.0\n",
      "31.0\n",
      "10.0\n",
      "62.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "11.0\n",
      "29.0\n",
      "11.0\n",
      "24.0\n",
      "37.0\n",
      "51.0\n",
      "49.0\n",
      "51.0\n",
      "32.0\n",
      "40.0\n",
      "41.0\n",
      "37.0\n",
      "13.0\n",
      "47.0\n",
      "31.0\n",
      "24.0\n",
      "26.0\n",
      "22.0\n",
      "12.0\n",
      "14.0\n",
      "15.0\n",
      "17.0\n",
      "15.0\n",
      "15.0\n",
      "16.0\n",
      "14.0\n",
      "18.0\n",
      "18.0\n",
      "80.0\n",
      "17.0\n",
      "100.0\n",
      "101.0\n",
      "104.0\n",
      "94.0\n",
      "107.0\n",
      "115.0\n",
      "134.0\n",
      "138.0\n",
      "158.0\n",
      "152.0\n",
      "186.0\n",
      "141.0\n",
      "131.0\n",
      "126.0\n",
      "119.0\n",
      "106.0\n",
      "118.0\n",
      "128.0\n",
      "112.0\n",
      "120.0\n",
      "121.0\n",
      "117.0\n",
      "122.0\n",
      "135.0\n",
      "158.0\n",
      "138.0\n",
      "114.0\n",
      "105.0\n",
      "125.0\n",
      "126.0\n",
      "136.0\n",
      "129.0\n",
      "124.0\n",
      "115.0\n",
      "128.0\n",
      "117.0\n",
      "118.0\n",
      "164.0\n",
      "103.0\n",
      "146.0\n",
      "110.0\n",
      "126.0\n",
      "117.0\n",
      "120.0\n",
      "124.0\n",
      "110.0\n",
      "166.0\n",
      "195.0\n",
      "100.0\n",
      "107.0\n",
      "196.0\n",
      "95.0\n",
      "92.0\n",
      "92.0\n",
      "95.0\n",
      "104.0\n",
      "92.0\n",
      "108.0\n",
      "114.0\n",
      "122.0\n",
      "123.0\n",
      "106.0\n",
      "117.0\n",
      "117.0\n",
      "116.0\n",
      "115.0\n",
      "109.0\n",
      "97.0\n",
      "105.0\n",
      "113.0\n",
      "97.0\n",
      "122.0\n",
      "103.0\n",
      "115.0\n",
      "114.0\n",
      "107.0\n",
      "118.0\n",
      "120.0\n",
      "129.0\n",
      "117.0\n",
      "116.0\n",
      "145.0\n",
      "353.0\n",
      "282.0\n",
      "316.0\n",
      "23.0\n",
      "92.0\n",
      "94.0\n",
      "91.0\n",
      "107.0\n",
      "103.0\n",
      "116.0\n",
      "146.0\n",
      "144.0\n",
      "120.0\n",
      "182.0\n",
      "188.0\n",
      "500.0\n",
      "405.0\n",
      "201.0\n",
      "247.0\n",
      "237.0\n",
      "338.0\n",
      "500.0\n",
      "500.0\n",
      "370.0\n",
      "500.0\n",
      "500.0\n",
      "133.0\n",
      "91.0\n",
      "365.0\n",
      "200.0\n",
      "41.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "213.0\n",
      "218.0\n",
      "202.0\n",
      "287.0\n",
      "96.0\n",
      "59.0\n",
      "24.0\n",
      "20.0\n",
      "18.0\n",
      "22.0\n",
      "16.0\n",
      "17.0\n",
      "12.0\n",
      "16.0\n",
      "29.0\n",
      "15.0\n",
      "11.0\n",
      "13.0\n",
      "15.0\n",
      "17.0\n",
      "15.0\n",
      "20.0\n",
      "25.0\n",
      "19.0\n",
      "19.0\n",
      "24.0\n",
      "24.0\n",
      "20.0\n",
      "19.0\n",
      "12.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "44.0\n",
      "21.0\n",
      "30.0\n",
      "117.0\n",
      "99.0\n",
      "53.0\n",
      "309.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "183.0\n",
      "500.0\n",
      "185.0\n",
      "139.0\n",
      "69.0\n",
      "151.0\n",
      "79.0\n",
      "64.0\n",
      "76.0\n",
      "98.0\n",
      "83.0\n",
      "85.0\n",
      "61.0\n",
      "42.0\n",
      "41.0\n",
      "54.0\n",
      "58.0\n",
      "61.0\n",
      "34.0\n",
      "106.0\n",
      "105.0\n",
      "75.0\n",
      "66.0\n",
      "365.0\n",
      "429.0\n",
      "500.0\n",
      "155.0\n",
      "134.0\n",
      "146.0\n",
      "318.0\n",
      "160.0\n",
      "170.0\n",
      "186.0\n",
      "244.0\n",
      "500.0\n",
      "500.0\n",
      "148.0\n",
      "500.0\n",
      "500.0\n",
      "200.0\n",
      "95.0\n",
      "179.0\n",
      "154.0\n",
      "404.0\n",
      "110.0\n",
      "15.0\n",
      "16.0\n",
      "500.0\n",
      "13.0\n",
      "17.0\n",
      "16.0\n",
      "16.0\n",
      "500.0\n",
      "405.0\n",
      "185.0\n",
      "138.0\n",
      "160.0\n",
      "86.0\n",
      "133.0\n",
      "58.0\n",
      "57.0\n",
      "54.0\n",
      "57.0\n",
      "18.0\n",
      "20.0\n",
      "16.0\n",
      "16.0\n",
      "22.0\n",
      "41.0\n",
      "15.0\n",
      "18.0\n",
      "14.0\n",
      "40.0\n",
      "19.0\n",
      "18.0\n",
      "18.0\n",
      "16.0\n",
      "15.0\n",
      "12.0\n",
      "14.0\n",
      "13.0\n",
      "14.0\n",
      "16.0\n",
      "14.0\n",
      "15.0\n",
      "14.0\n",
      "14.0\n",
      "13.0\n",
      "20.0\n",
      "12.0\n",
      "17.0\n",
      "14.0\n",
      "17.0\n",
      "15.0\n",
      "16.0\n",
      "15.0\n",
      "16.0\n",
      "12.0\n",
      "19.0\n",
      "17.0\n",
      "15.0\n",
      "13.0\n",
      "15.0\n",
      "16.0\n",
      "16.0\n",
      "17.0\n",
      "14.0\n",
      "13.0\n",
      "16.0\n",
      "15.0\n",
      "18.0\n",
      "16.0\n",
      "12.0\n",
      "14.0\n",
      "15.0\n",
      "14.0\n",
      "15.0\n",
      "12.0\n",
      "12.0\n",
      "13.0\n",
      "15.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "13.0\n",
      "16.0\n",
      "15.0\n",
      "13.0\n",
      "13.0\n",
      "12.0\n",
      "16.0\n",
      "18.0\n",
      "13.0\n",
      "15.0\n",
      "14.0\n",
      "12.0\n",
      "13.0\n",
      "16.0\n",
      "15.0\n",
      "16.0\n",
      "14.0\n",
      "13.0\n",
      "15.0\n",
      "16.0\n",
      "12.0\n",
      "15.0\n",
      "25.0\n",
      "23.0\n",
      "22.0\n",
      "25.0\n",
      "23.0\n",
      "27.0\n",
      "28.0\n",
      "25.0\n",
      "30.0\n",
      "30.0\n",
      "28.0\n",
      "26.0\n",
      "28.0\n",
      "27.0\n",
      "28.0\n",
      "32.0\n",
      "37.0\n",
      "32.0\n",
      "93.0\n",
      "41.0\n",
      "37.0\n",
      "36.0\n",
      "44.0\n",
      "100.0\n",
      "41.0\n",
      "47.0\n",
      "45.0\n",
      "39.0\n",
      "40.0\n",
      "42.0\n",
      "117.0\n",
      "49.0\n",
      "147.0\n",
      "116.0\n",
      "32.0\n",
      "36.0\n",
      "39.0\n",
      "43.0\n",
      "37.0\n",
      "43.0\n",
      "45.0\n",
      "150.0\n",
      "176.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "53.0\n",
      "23.0\n",
      "124.0\n",
      "500.0\n",
      "26.0\n",
      "35.0\n",
      "28.0\n",
      "28.0\n",
      "15.0\n",
      "17.0\n",
      "16.0\n",
      "15.0\n",
      "34.0\n",
      "20.0\n",
      "15.0\n",
      "21.0\n",
      "17.0\n",
      "21.0\n",
      "19.0\n",
      "28.0\n",
      "25.0\n",
      "39.0\n",
      "320.0\n",
      "54.0\n",
      "28.0\n",
      "37.0\n",
      "39.0\n",
      "187.0\n",
      "31.0\n",
      "25.0\n",
      "129.0\n",
      "32.0\n",
      "41.0\n",
      "28.0\n",
      "39.0\n",
      "39.0\n",
      "27.0\n",
      "37.0\n",
      "30.0\n",
      "107.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)  # Initialise le seed du pseudo-random\n",
    "np.random.seed(5)\n",
    "params = {'eps':0.01, 'epsDecay':0.99999, 'batch':100, 'C':100, 'lr':0.005, 'layers':[200], 'gamma':0.999, 'N':100000}\n",
    "agent = DQN(env, params)\n",
    "#agent = RandomAgent(env.action_space)\n",
    "\n",
    "outdir = outdir = 'cartpole-v0/random-agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "\n",
    "for i in range(500):\n",
    "    rsum = 0\n",
    "    obs = envm.reset()\n",
    "    reward = 0 \n",
    "    done = False \n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(obs, reward, done)\n",
    "        obs, reward, done, _ = envm.step(action)\n",
    "        agent.learn(obs, reward, done)\n",
    "        rsum += reward\n",
    "        if done:\n",
    "            break\n",
    "    print(rsum)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
