{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5 - Réseaux convolutifs 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "import csv\n",
    "import gzip\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "\n",
    "from datamaestro import Dataset, prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NB_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing et exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\"Batch\", [\"text\", \"labels\"])\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, text: torch.LongTensor, sizes: torch.LongTensor, labels: torch.LongTensor):\n",
    "        self.text = text\n",
    "        self.sizes = sizes\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.text[self.sizes[index]:self.sizes[index+1]], self.labels[index].item()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch):\n",
    "        data = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        return Batch(torch.nn.utils.rnn.pad_sequence(data, batch_first=True), torch.LongTensor(labels))\n",
    "\n",
    "\n",
    "def read(mode: str):\n",
    "    \"\"\"Process the dataset\n",
    "    \"\"\"\n",
    "    datapath = Path(mode)\n",
    "    if datapath.is_file():\n",
    "        with gzip.open(datapath, \"rb\") as fp:\n",
    "            return torch.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('wp1000.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read('train-1000.pth')\n",
    "test = read('test-1000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awww, that s a bummer You shoulda got David Carr of Third Day to do it ;D\n",
      "['▁A', 'www', ',', '▁that', '▁s', '▁a', '▁bu', 'mm', 'er', '▁You', '▁should', 'a', '▁got', '▁D', 'av', 'id', '▁C', 'ar', 'r', '▁of', '▁Th', 'ir', 'd', '▁Day', '▁to', '▁do', '▁it', '▁;', 'D']\n",
      "is upset that he can t update his Facebook by texting it and might cry as a result School today also Blah\n",
      "['▁is', '▁up', 's', 'et', '▁that', '▁he', '▁can', '▁t', '▁update', '▁his', '▁F', 'a', 'ce', 'b', 'ook', '▁by', '▁text', 'ing', '▁it', '▁and', '▁might', '▁cry', '▁as', '▁a', '▁re', 's', 'ult', '▁S', 'ch', 'o', 'ol', '▁today', '▁also', '▁B', 'la', 'h']\n",
      "I dived many times for the ball Managed to save 50 The rest go out of bounds\n",
      "['▁I', '▁di', 'v', 'ed', '▁many', '▁time', 's', '▁for', '▁the', '▁b', 'all', '▁M', 'an', 'age', 'd', '▁to', '▁s', 'a', 've', '▁5', '0', '▁The', '▁rest', '▁go', '▁out', '▁of', '▁b', 'ound', 's']\n",
      "my whole body feels itchy and like its on fire\n",
      "['▁my', '▁who', 'le', '▁bo', 'dy', '▁feel', 's', '▁it', 'ch', 'y', '▁and', '▁like', '▁its', '▁on', '▁f', 'i', 're']\n",
      "no, it s not behaving at all i m mad why am i here because I can t see you all over there\n",
      "['▁no', ',', '▁it', '▁s', '▁not', '▁be', 'h', 'av', 'ing', '▁at', '▁all', '▁i', '▁m', '▁m', 'ad', '▁why', '▁am', '▁i', '▁here', '▁because', '▁I', '▁can', '▁t', '▁see', '▁you', '▁all', '▁over', '▁there']\n",
      "not the whole crew\n",
      "['▁not', '▁the', '▁who', 'le', '▁c', 're', 'w']\n",
      "Need a hug\n",
      "['▁N', 'e', 'ed', '▁a', '▁hug']\n",
      "hey long time no see Yes Rains a bit ,only a bit LOL , I m fine thanks , how s you\n",
      "['▁hey', '▁long', '▁time', '▁no', '▁see', '▁Yes', '▁R', 'a', 'in', 's', '▁a', '▁bit', '▁', ',', 'on', 'ly', '▁a', '▁bit', '▁LOL', '▁', ',', '▁I', '▁m', '▁f', 'ine', '▁thanks', '▁', ',', '▁how', '▁s', '▁you']\n",
      "nope they didn t have it\n",
      "['▁no', 'pe', '▁they', '▁didn', '▁t', '▁have', '▁it']\n",
      "que me muera\n",
      "['▁', 'que', '▁me', '▁m', 'ue', 'ra']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(s.DecodeIds(train[i][0].tolist()))\n",
    "    print(s.EncodeAsPieces(s.DecodeIds(train[i][0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train, shuffle=True, collate_fn=train.collate, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test, shuffle=True, collate_fn=train.collate, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test success: 0.511\n"
     ]
    }
   ],
   "source": [
    "# Baseline test (majority class)\n",
    "print('Test success: %0.3f' % (np.mean([float(torch.sum(1 == b)) / len(b) for a, b in test_loader])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_max(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(my_max, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.max(x, self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_transpose(torch.nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(my_transpose, self).__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, self.dim1, self.dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = torch.nn.Sequential(\n",
    "        torch.nn.Embedding(1000, 50),\n",
    "        my_transpose(1, 2),\n",
    "        torch.nn.Conv1d(50, 50, 5),\n",
    "        torch.nn.MaxPool1d(5, 2),\n",
    "        torch.nn.Conv1d(50, 50, 5),\n",
    "        my_max(2),\n",
    "        torch.nn.Linear(50, 2),\n",
    "        torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complex = torch.nn.Sequential(\n",
    "        torch.nn.Embedding(1000, 50),\n",
    "        my_transpose(1, 2),\n",
    "        torch.nn.Conv1d(50, 50, 5),\n",
    "        torch.nn.MaxPool1d(5, 2),\n",
    "        torch.nn.Conv1d(50, 50, 5),\n",
    "        my_max(2),\n",
    "        torch.nn.Linear(50, 2),\n",
    "        torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48, 50])\n",
      "torch.Size([64, 50, 48])\n",
      "torch.Size([64, 50, 44])\n",
      "torch.Size([64, 50, 20])\n",
      "torch.Size([64, 50, 16])\n",
      "torch.Size([64, 50])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test shapes\n",
    "for x, y in data_loader:\n",
    "    print(x.shape)\n",
    "    for layer in model:\n",
    "        x = layer(x)\n",
    "        print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(params=model.parameters())\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning epoch 1\n",
      "Percentage of seen data: 0.00\n",
      "Test success: 0.50\n",
      "Percentage of seen data: 0.04\n",
      "Test success: 0.69\n",
      "Percentage of seen data: 0.08\n",
      "Test success: 0.69\n",
      "Percentage of seen data: 0.12\n",
      "Test success: 0.69\n",
      "Percentage of seen data: 0.16\n",
      "Test success: 0.72\n",
      "Percentage of seen data: 0.20\n",
      "Test success: 0.73\n",
      "Percentage of seen data: 0.24\n",
      "Test success: 0.71\n",
      "Percentage of seen data: 0.28\n",
      "Test success: 0.71\n",
      "Percentage of seen data: 0.32\n",
      "Test success: 0.71\n",
      "Percentage of seen data: 0.36\n",
      "Test success: 0.73\n",
      "Percentage of seen data: 0.40\n",
      "Test success: 0.75\n",
      "Percentage of seen data: 0.44\n",
      "Test success: 0.72\n",
      "Percentage of seen data: 0.48\n",
      "Test success: 0.73\n",
      "Percentage of seen data: 0.52\n",
      "Test success: 0.71\n",
      "Percentage of seen data: 0.56\n",
      "Test success: 0.72\n",
      "Percentage of seen data: 0.60\n",
      "Test success: 0.73\n",
      "Percentage of seen data: 0.64\n",
      "Test success: 0.73\n",
      "Percentage of seen data: 0.68\n",
      "Test success: 0.72\n",
      "Percentage of seen data: 0.72\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.76\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.80\n",
      "Test success: 0.73\n",
      "Percentage of seen data: 0.84\n",
      "Test success: 0.76\n",
      "Percentage of seen data: 0.88\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.92\n",
      "Test success: 0.75\n",
      "Percentage of seen data: 0.96\n",
      "Test success: 0.76\n",
      "Beginning epoch 2\n",
      "Percentage of seen data: 0.00\n",
      "Test success: 0.75\n",
      "Percentage of seen data: 0.04\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.08\n",
      "Test success: 0.76\n",
      "Percentage of seen data: 0.12\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.16\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.20\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.24\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.28\n",
      "Test success: 0.74\n",
      "Percentage of seen data: 0.32\n",
      "Test success: 0.77\n",
      "Percentage of seen data: 0.36\n",
      "Test success: 0.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c07fc0120c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "losses = []\n",
    "for i in range(NB_EPOCH):\n",
    "    print('Beginning epoch %d' %(i+1))\n",
    "    j = 0\n",
    "    for x, y in data_loader:\n",
    "        if j % 1000 == 0:\n",
    "            print('Percentage of seen data: %0.2f' %(j*BATCH_SIZE / train.__len__()))\n",
    "            print('Test success: %0.2f' % (np.mean([float(torch.sum(torch.max(model(a), 1)[1] == b)) / len(b) for a, b in test_loader])))\n",
    "            writer.add_scalar('Loss/test', l, j)\n",
    "    # each time: we see batch * length data\n",
    "        optim.zero_grad()\n",
    "        l = loss(model(x), y)\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        writer.add_scalar('Loss/train', l, j)\n",
    "        losses.append(l)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
