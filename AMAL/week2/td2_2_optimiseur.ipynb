{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAL - TP 2 - Graphes de calcul, autograd et modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'automatiser les fonctions codées à la main lors du TP 1. On le fait en 3 étapes :\n",
    "+ 1. Calcul automatique du gradient par autograd\n",
    "+ 2. Descente de gradient automatique avec la classe `torch.optim`\n",
    "+ 3. Modules préprogrammés et conteneurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, on implémente le point 2. La partie intéressante se situe dans la fonction `run_gradient_descent`, en particulier les lignes suivantes\n",
    "\n",
    "```python\n",
    "optim = torch.optim.SGD(params=[w,b],lr=eps)\n",
    "...\n",
    "optim.step()\n",
    "optim.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "from datamaestro import prepare_dataset \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation & test des classes Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"Very simplified context object\"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par implémenter le cas unidimensionnel avant d'implémenter le cas multi-dimensionnel. Pour chaque classe, on la teste juste après sa définition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear1(Function):\n",
    "    \"\"\"Implementation of f(x, w, b) = <x, w> + b\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx,x, w, b):\n",
    "        ## Calcul la sortie du module\n",
    "        ctx.save_for_backward(x,w)\n",
    "        return torch.dot(x, w) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport a chaque groupe d'entrées\n",
    "        x,w = ctx.saved_tensors\n",
    "        return grad_output * w , grad_output * x, grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_linear1():\n",
    "    ## test de la fonction\n",
    "    estimator = Linear1()\n",
    "    ctx_estimator = Context()\n",
    "    x = torch.randn(5,requires_grad=True,dtype=torch.float64)\n",
    "    w = torch.randn(5,requires_grad=True,dtype=torch.float64)\n",
    "    b = torch.randn(1,requires_grad=True,dtype=torch.float64)\n",
    "    output = estimator.forward(ctx_estimator,x,w, b)\n",
    "\n",
    "    estimator_grad = estimator.backward(ctx_estimator,1)\n",
    "\n",
    "    ## Pour tester le gradient \n",
    "    estimator_check = Linear1.apply\n",
    "    return torch.autograd.gradcheck(estimator_check,(x,w, b))\n",
    "\n",
    "test_linear1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBatch(Function):\n",
    "    \"\"\"Implementation of f(X, w, b) = X.dot(w) + b\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, w, b):\n",
    "        ## Calcul la sortie du module\n",
    "        ctx.save_for_backward(X,w)\n",
    "        return torch.mm(X, torch.transpose(w, 0, 1)) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport a chaque groupe d'entrées\n",
    "        X, w = ctx.saved_tensors\n",
    "        return torch.mm(grad_output, w) , torch.mm(torch.transpose(grad_output, 0, 1), X), torch.sum(grad_output, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = 10\n",
    "def test_linearBatch():\n",
    "    ## Test de la fonction \n",
    "    estimator = LinearBatch()\n",
    "    ctx_estimator = Context()\n",
    "    x = torch.randn(b_size, 5,requires_grad=True,dtype=torch.float64)\n",
    "    w = torch.randn(1, 5,requires_grad=True,dtype=torch.float64)\n",
    "    b = torch.randn(1,requires_grad=True,dtype=torch.float64)\n",
    "    output = estimator.forward(ctx_estimator,x,w, b)\n",
    "\n",
    "    grad_output = torch.randn(b_size, 1,requires_grad=True,dtype=torch.float64)\n",
    "    estimator_grad = estimator.backward(ctx_estimator, grad_output)\n",
    "\n",
    "    ## Pour tester le gradient \n",
    "    estimator_check = LinearBatch.apply\n",
    "    return torch.autograd.gradcheck(estimator_check,(x,w, b))\n",
    "\n",
    "test_linearBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Function):\n",
    "    \"\"\"Implementation of f(yhat, y) = ||yhat-y||^2\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ## Calcul la sortie du module\n",
    "        ctx.save_for_backward(yhat,y)\n",
    "        return torch.norm(yhat-y, p=2)**2 / len(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport a chaque groupe d'entrées\n",
    "        yhat,y = ctx.saved_tensors\n",
    "        return 2 * grad_output * (yhat-y)/ len(y) ,  -2 * grad_output * (yhat-y)  / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_MSE():\n",
    "    mse = MSE()\n",
    "    ctx_mse = Context()\n",
    "    y = torch.randn(2,requires_grad=True,dtype=torch.float64)\n",
    "    yhat = torch.randn(2,requires_grad=True,dtype=torch.float64)\n",
    "    output = mse.forward(ctx_mse,yhat,y)\n",
    "    mse_grad = mse.backward(ctx_mse,1)\n",
    "    ## Pour tester le gradient \n",
    "    mse_check = mse.apply\n",
    "    return torch.autograd.gradcheck(mse_check,(yhat, y))\n",
    "\n",
    "test_MSE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de l'algorithme de descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `run_gradient_descent` permet d'implémenter les trois variantes de descente de gradient :\n",
    "+ Gradient batch: `n_batch` = 1\n",
    "+ Gradient stochastique: `n_batch` = N où N est le nombre de lignes de données\n",
    "+ Gradient mini-batch: 1 < `n_batch` < N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent(data, nb_iteration, n_batch, writer=False):\n",
    "    \n",
    "    N = data.shape[0]   # nombre de données\n",
    "    n_attribute = data.shape[1]-1\n",
    "    \n",
    "    if writer:\n",
    "        writer = SummaryWriter(log_dir='runs/nb_batch=' + str(n_batch) + 'nb_pass=' + str(nb_iteration/n_batch) + '_timestamp=' + str(int(time.time())))\n",
    "        \n",
    "    # Initialisation aléatoire des paramètres w et b\n",
    "    w = torch.nn.Parameter(torch.randn(1, n_attribute,dtype=torch.float64))\n",
    "    b = torch.nn.Parameter(torch.randn(1,dtype=torch.float64))\n",
    "    \n",
    "    eps = 0.05 # Pas de la descente de gradient\n",
    "    optim = torch.optim.SGD(params=[w,b],lr=eps)\n",
    "    \n",
    "    estimator = LinearBatch()\n",
    "    ctx_estimator = Context()\n",
    "    mse = MSE()\n",
    "    ctx_mse = Context()\n",
    "\n",
    "    L_train = 0     # perte pour la partie train\n",
    "    for k in range(nb_iteration):\n",
    "        \n",
    "        # Choix des lignes des données qui servent pour cette itération, parmi la première moitié des données\n",
    "        if n_batch == N: # stochastic \n",
    "            idx = [np.random.randint(0, N/2)]\n",
    "        else:   # batch or mini-batch\n",
    "            idx_min = int(N/n_batch * k) % N\n",
    "            idx_max = int(N/n_batch * (k+1)) % N\n",
    "            if idx_min < idx_max:\n",
    "                idx = range(idx_min, idx_max)\n",
    "            else:\n",
    "                idx = list(range(idx_min, N)) + list(range(0, idx_max))\n",
    "            \n",
    "        X = torch.from_numpy(data[idx, :-1])\n",
    "        y = torch.from_numpy(data[idx][: , [-1]])\n",
    "        loss = mse.forward(ctx_mse, estimator.forward(ctx_estimator, X, w, b), y)\n",
    "        L_train = (k*L_train + loss) / (k+1)    # update L_train\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    \n",
    "        if k % 10 == 0 and writer:\n",
    "            # Pour comparer les différente méthodes, on compte le nombre de lignes de données utilisées à chaque itération\n",
    "            writer.add_scalar('Loss/train', L_train/len(idx), k*len(idx))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        L_test = 0  # perte pour la partie test\n",
    "        for k in range(int(N/2)):     # on teste sur la seconde moitié du data set\n",
    "            x = torch.from_numpy(data[[int(N/2) + k], :-1])\n",
    "            y = torch.from_numpy(data[[int(N/2) + k]][: , [-1]])\n",
    "            L_test = (k*L_test + mse.forward(ctx_mse, estimator.forward(ctx_estimator, x, w, b), y)) / (k+1)\n",
    "            if writer:\n",
    "                writer.add_scalar('Loss/test', L_test, k)\n",
    "    \n",
    "    print('Test loss: ' + str(L_test.item()))\n",
    "    if writer:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec les données de Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pour telecharger le dataset Boston\n",
    "ds=prepare_dataset(\"edu.uci.boston\")\n",
    "fields, data =ds.files.data() \n",
    "N = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour randomizer les données et diminuer le conditionnement\n",
    "np.random.shuffle(data)\n",
    "data = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07727138162464059\n",
      "Test loss: 0.009941845629447284\n",
      "Test loss: 0.014954277493030738\n"
     ]
    }
   ],
   "source": [
    "# On compare les trois variantes (batch / stochastic / mini-batch) en utilisant à chaque fois au total `n_pass` fois les données\n",
    "# Le nombre d'itérations n'est donc pas le même selon les variantes\n",
    "n_pass = 100\n",
    "run_gradient_descent(data, n_pass, 1, writer=True)    # batch\n",
    "run_gradient_descent(data, n_pass*N, N, writer=True)  # stochastic\n",
    "n_batch = 10\n",
    "run_gradient_descent(data, n_pass*n_batch, n_batch, writer=True)   # mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
