{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAL - TP 2 - Graphes de calcul, autograd et modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'automatiser les fonctions codées à la main lors du TP 1. On le fait en 3 étapes :\n",
    "+ 1. Calcul automatique du gradient par autograd\n",
    "+ 2. Descente de gradient automatique avec la classe `torch.optim`\n",
    "+ 3. Modules préprogrammés et conteneurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, on implémente le point 3. La partie intéressante se situe dans la fonction `run_gradient_descent`, en particulier les lignes suivantes\n",
    "\n",
    "```python\n",
    "mse = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(n_attribute, nb_neurons),\n",
    "      torch.nn.Tanh(),\n",
    "      torch.nn.Linear(nb_neurons, 1)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "from datamaestro import prepare_dataset \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de l'algorithme de descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `run_gradient_descent` permet d'implémenter les trois variantes de descente de gradient :\n",
    "+ Gradient batch: `n_batch` = 1\n",
    "+ Gradient stochastique: `n_batch` = N où N est le nombre de lignes de données\n",
    "+ Gradient mini-batch: 1 < `n_batch` < N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent(data, nb_iteration, n_batch, nb_neurons, write=False):\n",
    "    \n",
    "    N = data.shape[0]   # nombre de données\n",
    "    n_attribute = data.shape[1]-1\n",
    "    \n",
    "    if write:\n",
    "        writer = SummaryWriter(log_dir='runs/nb_batch=' + str(n_batch) + 'nb_pass=' + str(nb_iteration/n_batch) + '_timestamp=' + str(int(time.time())))\n",
    "    \n",
    "    eps = 0.05 # Pas de la descente de gradient\n",
    "    \n",
    "    mse = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(n_attribute, nb_neurons),\n",
    "          torch.nn.Tanh(),\n",
    "          torch.nn.Linear(nb_neurons, 1)\n",
    "        )\n",
    "    \n",
    "    optim = torch.optim.SGD(params=model.parameters(),lr=eps)\n",
    "    optim.zero_grad()\n",
    "\n",
    "    L_train = 0     # perte pour la partie train\n",
    "    for k in range(nb_iteration):\n",
    "        \n",
    "        # Choix des lignes des données qui servent pour cette itération\n",
    "        if n_batch == N: # stochastic \n",
    "            idx = [np.random.randint(0, N//2)]\n",
    "        else:   # batch or mini-batch\n",
    "            idx_min = int(N/n_batch * k) % N\n",
    "            idx_max = int(N/n_batch * (k+1)) % N\n",
    "            if idx_min < idx_max:\n",
    "                idx = range(idx_min, idx_max)\n",
    "            else:\n",
    "                idx = list(range(idx_min, N)) + list(range(0, idx_max))\n",
    "            \n",
    "        X = torch.from_numpy(data[idx, :-1])\n",
    "        y = torch.from_numpy(data[idx][: , [-1]])\n",
    "        loss = mse(model(X.float()), y.float())\n",
    "        L_train = (k*L_train + loss) / (k+1)    # update L_train\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    \n",
    "        if k % 10 == 0 and write:\n",
    "            # Pour comparer les différente méthodes, on compte le nombre de lignes de données utilisées à chaque itération\n",
    "            writer.add_scalar('Loss/train', L_train/len(idx), k*len(idx))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        L_test = 0  # perte pour la partie test\n",
    "        for k in range(int(N/2)):     # on teste sur la seconde moitié du data set\n",
    "            x = torch.from_numpy(data[[int(N/2) + k], :-1])\n",
    "            y = torch.from_numpy(data[[int(N/2) + k]][: , [-1]])\n",
    "            loss = mse(model(x.float()), y.float())\n",
    "            L_test = (k*L_test + loss) / (k+1)\n",
    "            if write:\n",
    "                writer.add_scalar('Loss/test', L_test, k)\n",
    "    \n",
    "    print('Test loss: ' + str(L_test.item()))\n",
    "    if write:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test avec les données de Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pour telecharger le dataset Boston\n",
    "ds=prepare_dataset(\"edu.uci.boston\")\n",
    "fields, data =ds.files.data() \n",
    "N = data.shape[0]\n",
    "n_attribute = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour randomizer les données et diminuer le conditionnement\n",
    "np.random.shuffle(data)\n",
    "data = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02113339677453041\n",
      "Test loss: 0.006567567121237516\n",
      "Test loss: 0.010460889898240566\n"
     ]
    }
   ],
   "source": [
    "# On compare les trois variantes (batch / stochastic / mini-batch) en utilisant à chaque fois au total `n_pass` fois les données\n",
    "# Le nombre d'itérations n'est donc pas le même selon les variantes\n",
    "n_pass = 100\n",
    "nb_neurons = 10\n",
    "run_gradient_descent(data, n_pass, 1, nb_neurons, write=True)    # batch\n",
    "run_gradient_descent(data, n_pass*N, N, nb_neurons, write=True)  # stochastic\n",
    "n_batch = 10\n",
    "run_gradient_descent(data, n_pass*n_batch, n_batch, nb_neurons, write=True)   # mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
